{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Feiquanl/CS5100Project/blob/main/accuracybeforeSentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlBXDVyuJt8o"
      },
      "source": [
        "Sentiment Analysis\n",
        "- to extract subjective information and rate the sentiment according to review Text and compare to orignoal overall score to find out accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ94Ro0oD-Hr"
      },
      "outputs": [],
      "source": [
        "import os, json, gzip\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import csv\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohTWPtsrmo4z",
        "outputId": "1842099d-d35a-4cff-90df-b8dd16be63fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Book_1_100.csv')\n",
        "# Check the population size (number of rows) in your DataFrame\n",
        "print(\"Population size:\", len(df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeljSq5aKCK7",
        "outputId": "5db3c0c4-5693-4a8f-c483-f2dd4d1417ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Population size: 89635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbzPm4ZTJt8q"
      },
      "source": [
        "We only forcus on overall and reviewText column, and we can see there is imbalanced data from overall 1 to overall 5. So we need to sort out an equal number of obervation for each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp3_pIQKIcTn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b42ea1f-7987-4e73-f9a1-26b1b4fedef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Population size: 89603\n",
            "overall\n",
            "5.0    69058\n",
            "4.0    11691\n",
            "3.0     4524\n",
            "1.0     2232\n",
            "2.0     2098\n",
            "Name: count, dtype: int64\n",
            "Population size: 89603\n"
          ]
        }
      ],
      "source": [
        "# drop any rows w/ missing values\n",
        "df = df.dropna(subset=[\"reviewText\"])\n",
        "print(\"Population size:\", len(df))\n",
        "#discover the actual counts\n",
        "print(df.overall.value_counts())\n",
        "\n",
        "print(\"Population size:\", len(df))\n",
        "\n",
        "# set sample size to labels w/ minimum count\n",
        "sample_size = 7192\n",
        "# Collect samples for each class in a list\n",
        "#samples_list = [df[df.overall == label].sample(n=sample_size, random_state=1) for label in df.overall.unique()]\n",
        "# Specify the labels you're interested in\n",
        "target_labels = [4, 5]\n",
        "\n",
        "# Collect samples for each targeted class in a list\n",
        "samples_list = [df[df.overall == label].sample(n=sample_size, random_state=1) for label in target_labels]\n",
        "# Concatenate all the samples into one DataFrame\n",
        "df_equal_overall = pd.concat(samples_list, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAc1dTGFJt8q"
      },
      "source": [
        "Fundamental Preprocess for NLP : lowercasing, punctuations removal, and removal of stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxmFNVnZK4Lm"
      },
      "outputs": [],
      "source": [
        "stopwords_list = stopwords.words('english')\n",
        "\n",
        "def ReviewProcessing(df):\n",
        "  # remove non alphanumeric\n",
        "  df['review_cleaned'] = df.reviewText.str.replace('[^a-zA-Z0-9 ]', '')\n",
        "  # lowercase\n",
        "  df.review_cleaned = df.review_cleaned.str.lower()\n",
        "  # split into list\n",
        "  df.review_cleaned = df.review_cleaned.str.split(' ')\n",
        "  # remove stopwords\n",
        "  df.review_cleaned = df.review_cleaned.apply(lambda x: [item for item in x if item not in stopwords_list])\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9Z8I5JTJt8r"
      },
      "source": [
        "Lemmatiazation for NLP: the process of grouping inflected words into a root word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBSJh6LDK5nC"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(word):\n",
        "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "  tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "  return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "def get_lemmatize(sent):\n",
        "  return \" \".join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sent)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDHw55bpK8if",
        "tags": []
      },
      "outputs": [],
      "source": [
        "clean_data = ReviewProcessing(df_equal_overall)\n",
        "clean_data.review_cleaned = clean_data.review_cleaned.apply(' '.join)\n",
        "clean_data['review_cleaned_lemmatized'] = clean_data.review_cleaned.apply(get_lemmatize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dktb_tE5Jt8r"
      },
      "source": [
        "Implementing TF-IDF Weighting : to measure how important or relevant a term is within the sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgsSkSBrabb_",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb = Pipeline([('vectorize', CountVectorizer(ngram_range=(1, 2))),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JFU1fDPJt8r"
      },
      "source": [
        "Stochastic Gradient Descent: SGD algorithm computes the minimum of the coust function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PA2XdDtOchmE",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', SGDClassifier()),\n",
        "               ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytptV332Jt8s"
      },
      "source": [
        "Logistic Regression Classifier: categorizing dazta into binary or multiplr groups with discrete prediction value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGQyQ3n4ckQd"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', LogisticRegression(max_iter=500)),\n",
        "               ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gECSZ2HbJt8s"
      },
      "source": [
        "Split dataset into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCWm1ga7cn7D"
      },
      "outputs": [],
      "source": [
        "x = clean_data['review_cleaned_lemmatized']\n",
        "y = clean_data['overall']\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y,\n",
        "                                                    test_size=0.2, stratify=y, random_state = 44)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_tD7UE7Jt8s"
      },
      "source": [
        "Return the accuracy socre, confusion matrix and classification repost for each report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5aoew6vcq2t",
        "outputId": "008f4124-d7bd-4481-866a-a7a380e19a17",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6979492526937783\n",
            "0.7257559958289885\n",
            "0.7201946472019465\n"
          ]
        }
      ],
      "source": [
        "# Naive Bayes\n",
        "nb.fit(X_train, y_train)\n",
        "y_pred_nb = nb.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred_nb))\n",
        "#print(confusion_matrix(y_test, y_pred_nb))\n",
        "#print(classification_report(y_test, y_pred_nb))\n",
        "\n",
        "# SGD Classifier\n",
        "sgd.fit(X_train, y_train)\n",
        "y_pred_sgd = sgd.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred_sgd))\n",
        "#print(confusion_matrix(y_test, y_pred_sgd))\n",
        "#print(classification_report(y_test, y_pred_sgd))\n",
        "\n",
        "# Logistic Regression\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_log = logreg.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred_log))\n",
        "#print(confusion_matrix(y_test, y_pred_log))\n",
        "#print(classification_report(y_test, y_pred_log))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h5I-OLqcu7A",
        "outputId": "4e8263ea-8d8b-48c1-80af-5c83e348dcd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "Pipeline(steps=[('vect', CountVectorizer(ngram_range=(1, 2))),\n",
            "                ('tfidf', TfidfTransformer()),\n",
            "                ('clf', LogisticRegression(C=1, max_iter=500))])\n",
            "0.7101765144984749\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         4.0       0.71      0.74      0.73      1439\n",
            "         5.0       0.73      0.70      0.71      1438\n",
            "\n",
            "    accuracy                           0.72      2877\n",
            "   macro avg       0.72      0.72      0.72      2877\n",
            "weighted avg       0.72      0.72      0.72      2877\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid=[{'clf__solver': ['lbfgs', 'sag', 'saga'],\n",
        "       'clf__C': [0.01, 0.1, 1]}]\n",
        "lr = GridSearchCV(logreg, param_grid = grid, cv = 5, scoring='accuracy', verbose = 1, n_jobs = -1)\n",
        "best_model = lr.fit(X_train, y_train)\n",
        "\n",
        "print(best_model.best_estimator_)\n",
        "print(best_model.best_score_)\n",
        "\n",
        "y_pred_grid = best_model.predict(X_test)\n",
        "#print(confusion_matrix(y_test, y_pred_grid))\n",
        "print(classification_report(y_test, y_pred_grid))\n",
        "#print(accuracy_score(y_test, y_pred_grid))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}